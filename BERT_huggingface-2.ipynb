{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMuYd14gx0XW2MxlNfjBiXf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MoronSlayer/Deep-Learning-Projects/blob/main/BERT_huggingface-2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/huggingface/transformers.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LnrrOyRLt9F4",
        "outputId": "62be3945-d212-47af-fb5c-a23b363792ed"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 109695, done.\u001b[K\n",
            "remote: Counting objects: 100% (299/299), done.\u001b[K\n",
            "remote: Compressing objects: 100% (179/179), done.\u001b[K\n",
            "remote: Total 109695 (delta 143), reused 208 (delta 93), pack-reused 109396\u001b[K\n",
            "Receiving objects: 100% (109695/109695), 102.20 MiB | 29.68 MiB/s, done.\n",
            "Resolving deltas: 100% (81130/81130), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOycEm0TF-DW",
        "outputId": "7ffbc98c-cb23-49ca-c74d-108ff5a26f0d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.22.1-py3-none-any.whl (4.9 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.9 MB 38.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.6 MB 50.8 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.9.0\n",
            "  Downloading huggingface_hub-0.9.1-py3-none-any.whl (120 kB)\n",
            "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 120 kB 78.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.9.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.9.1 tokenizers-0.12.1 transformers-4.22.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b2Nv7ajKu8lD",
        "outputId": "6638808f-7576-478f-e1a1-b317ac622538"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: transformers\n",
            "Version: 4.22.1\n",
            "Summary: State-of-the-art Machine Learning for JAX, PyTorch and TensorFlow\n",
            "Home-page: https://github.com/huggingface/transformers\n",
            "Author: The Hugging Face team (past and future) with the help of all our contributors (https://github.com/huggingface/transformers/graphs/contributors)\n",
            "Author-email: transformers@huggingface.co\n",
            "License: Apache\n",
            "Location: /usr/local/lib/python3.7/dist-packages\n",
            "Requires: tqdm, huggingface-hub, requests, importlib-metadata, packaging, regex, tokenizers, pyyaml, filelock, numpy\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HAhuo56YFinN"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer\n",
        "bert_cased_tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertConfig, BertForPreTraining\n",
        "config = BertConfig()\n",
        "model = BertForPreTraining(config)"
      ],
      "metadata": {
        "id": "UbL5Wz4pGMNB"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pW2QS3WB-0EH",
        "outputId": "de8f25f8-cd6a-4c9a-f895-8d6a39b022a3"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertConfig {\n",
              "  \"architectures\": [\n",
              "    \"BertForPreTraining\"\n",
              "  ],\n",
              "  \"attention_probs_dropout_prob\": 0.1,\n",
              "  \"classifier_dropout\": null,\n",
              "  \"hidden_act\": \"gelu\",\n",
              "  \"hidden_dropout_prob\": 0.1,\n",
              "  \"hidden_size\": 768,\n",
              "  \"initializer_range\": 0.02,\n",
              "  \"intermediate_size\": 3072,\n",
              "  \"layer_norm_eps\": 1e-12,\n",
              "  \"max_position_embeddings\": 512,\n",
              "  \"model_type\": \"bert\",\n",
              "  \"num_attention_heads\": 12,\n",
              "  \"num_hidden_layers\": 12,\n",
              "  \"pad_token_id\": 0,\n",
              "  \"position_embedding_type\": \"absolute\",\n",
              "  \"torch_dtype\": \"float32\",\n",
              "  \"transformers_version\": \"4.22.1\",\n",
              "  \"type_vocab_size\": 2,\n",
              "  \"use_cache\": true,\n",
              "  \"vocab_size\": 30522\n",
              "}"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TextDatasetForNextSentencePrediction\n",
        "dataset = TextDatasetForNextSentencePrediction(\n",
        "    tokenizer=bert_cased_tokenizer,\n",
        "    file_path=\"/content/sample_data/dataset\",\n",
        "    block_size = 2\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQzMEaelGTzF",
        "outputId": "c1e21813-39ec-4f10-98a8-510bfd8ca61a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/transformers/data/datasets/language_modeling.py:366: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "USRTmx7cGwJ1",
        "outputId": "71b4cfdc-7b3a-445f-c328-ad80f4054afe"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3NRcI0SGy1J",
        "outputId": "7894d42f-8e95-4c55-ddf1-60060b3855d1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([  101,  2019,  6207,  4212,  2104,  1996,  3392,  1012,   102,  1037,\n",
              "          4248,  2829,  4419, 14523,  2058,  1996, 13971,  3899,  1012,   102]),\n",
              " 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
              " 'next_sentence_label': tensor(1)}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "\n",
        "example = \"Newton invents calculus.\"\n",
        "\n",
        "print({x : tokenizer.encode(x, add_special_tokens=False) for x in example.split()})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fsgF8K4TJe-X",
        "outputId": "6079611b-a6f6-416a-fb22-65d748bfee0f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Newton': [8446], 'invents': [1999, 15338, 2015], 'calculus.': [19276, 1012]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "example = \"An apple falls under the tree.\"\n",
        "print({x : tokenizer.encode(x, add_special_tokens=False) for x in example.split()})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Olbfg501KFkj",
        "outputId": "c425e207-ca4e-4ecd-bf27-fb48e1226e6a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'An': [2019], 'apple': [6207], 'falls': [4212], 'under': [2104], 'the': [1996], 'tree.': [3392, 1012]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
        "example = \"A quick brown fox jumps over the lazy dog.\"\n",
        "print({x : tokenizer.encode(x, add_special_tokens=False) for x in example.split()})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZX3th_bKwAd",
        "outputId": "9a1e8d2f-570d-4a3a-cfcc-cfce053e96e3"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'A': [1037], 'quick': [4248], 'brown': [2829], 'fox': [4419], 'jumps': [14523], 'over': [2058], 'the': [1996], 'lazy': [13971], 'dog.': [3899, 1012]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(8):\n",
        "  print(len(bert_cased_tokenizer.convert_ids_to_tokens(dataset[i]['input_ids'].tolist())))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nln-sCuLL4EX",
        "outputId": "e5247d80-bf5d-493e-a8f5-900211f3f22c"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "49\n",
            "70\n",
            "70\n",
            "70\n",
            "70\n",
            "49\n",
            "46\n",
            "62\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[7]['next_sentence_label']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kNqfRuFGNSIR",
        "outputId": "84131048-ac46-40e5-bf13-8cc90e9556ea"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "data_collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=bert_cased_tokenizer, \n",
        "    mlm=True,\n",
        "    mlm_probability= 0.15\n",
        ")"
      ],
      "metadata": {
        "id": "2HpNy1KANrhy"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir= \"/path/to/output/dir/for/training/arguments\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=10000,\n",
        "    per_gpu_train_batch_size= 16,\n",
        "    save_steps=100,\n",
        "    save_total_limit=2,\n",
        "    prediction_loss_only=True,\n",
        "    do_eval = True,\n",
        "    logging_steps=50,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=data_collator,\n",
        "    train_dataset=dataset,\n",
        "    eval_dataset = dataset, \n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "trainer.save_model(\"path/to/your/model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rllNqWopzv9q",
        "outputId": "8da417eb-490d-4b0a-af77-e0c15ad024d8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n",
            "***** Running training *****\n",
            "  Num examples = 8\n",
            "  Num Epochs = 10000\n",
            "  Instantaneous batch size per device = 8\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "  Gradient Accumulation steps = 1\n",
            "  Total optimization steps = 10000\n",
            "Using deprecated `--per_gpu_train_batch_size` argument which will be removed in a future version. Using `--per_device_train_batch_size` is preferred.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2608' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 2608/10000 09:15 < 26:15, 4.69 it/s, Epoch 2607/10000]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>8.005200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>4.702000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>2.786400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.545300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.665400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.235700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.079200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.025500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.013700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.010100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.007800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.006500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>0.005500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.004900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>0.004400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.003800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>850</td>\n",
              "      <td>0.003400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.003100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>950</td>\n",
              "      <td>0.002900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.002600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1050</td>\n",
              "      <td>0.002400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.002200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1150</td>\n",
              "      <td>0.002100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.001900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1250</td>\n",
              "      <td>0.001800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.001700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1350</td>\n",
              "      <td>0.001600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.001500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1450</td>\n",
              "      <td>0.001400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.001400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1550</td>\n",
              "      <td>0.001300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.001200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1650</td>\n",
              "      <td>0.001200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1700</td>\n",
              "      <td>0.001100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1750</td>\n",
              "      <td>0.001000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1800</td>\n",
              "      <td>0.001000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1850</td>\n",
              "      <td>0.000900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1900</td>\n",
              "      <td>0.000900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1950</td>\n",
              "      <td>0.000900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>0.080600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2050</td>\n",
              "      <td>0.060500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2100</td>\n",
              "      <td>0.050400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2150</td>\n",
              "      <td>0.018400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2200</td>\n",
              "      <td>0.002400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2250</td>\n",
              "      <td>0.001100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2300</td>\n",
              "      <td>0.000800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2350</td>\n",
              "      <td>0.000700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2400</td>\n",
              "      <td>0.000700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2450</td>\n",
              "      <td>0.000600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.000600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2550</td>\n",
              "      <td>0.000500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2600</td>\n",
              "      <td>0.000500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Saving model checkpoint to /path/to/output/dir/for/training/arguments/checkpoint-100\n",
            "Configuration saved in /path/to/output/dir/for/training/arguments/checkpoint-100/config.json\n",
            "Model weights saved in /path/to/output/dir/for/training/arguments/checkpoint-100/pytorch_model.bin\n",
            "Deleting older checkpoint [/path/to/output/dir/for/training/arguments/checkpoint-400] due to args.save_total_limit\n",
            "Saving model checkpoint to /path/to/output/dir/for/training/arguments/checkpoint-200\n",
            "Configuration saved in /path/to/output/dir/for/training/arguments/checkpoint-200/config.json\n",
            "Model weights saved in /path/to/output/dir/for/training/arguments/checkpoint-200/pytorch_model.bin\n",
            "Deleting older checkpoint [/path/to/output/dir/for/training/arguments/checkpoint-500] due to args.save_total_limit\n",
            "Saving model checkpoint to /path/to/output/dir/for/training/arguments/checkpoint-300\n",
            "Configuration saved in /path/to/output/dir/for/training/arguments/checkpoint-300/config.json\n",
            "Model weights saved in /path/to/output/dir/for/training/arguments/checkpoint-300/pytorch_model.bin\n",
            "Deleting older checkpoint [/path/to/output/dir/for/training/arguments/checkpoint-100] due to args.save_total_limit\n",
            "Saving model checkpoint to /path/to/output/dir/for/training/arguments/checkpoint-400\n",
            "Configuration saved in /path/to/output/dir/for/training/arguments/checkpoint-400/config.json\n",
            "Model weights saved in /path/to/output/dir/for/training/arguments/checkpoint-400/pytorch_model.bin\n",
            "Deleting older checkpoint [/path/to/output/dir/for/training/arguments/checkpoint-200] due to args.save_total_limit\n",
            "Saving model checkpoint to /path/to/output/dir/for/training/arguments/checkpoint-500\n",
            "Configuration saved in /path/to/output/dir/for/training/arguments/checkpoint-500/config.json\n",
            "Model weights saved in /path/to/output/dir/for/training/arguments/checkpoint-500/pytorch_model.bin\n",
            "Deleting older checkpoint [/path/to/output/dir/for/training/arguments/checkpoint-300] due to args.save_total_limit\n",
            "Saving model checkpoint to /path/to/output/dir/for/training/arguments/checkpoint-600\n",
            "Configuration saved in /path/to/output/dir/for/training/arguments/checkpoint-600/config.json\n",
            "Model weights saved in /path/to/output/dir/for/training/arguments/checkpoint-600/pytorch_model.bin\n",
            "Deleting older checkpoint [/path/to/output/dir/for/training/arguments/checkpoint-400] due to args.save_total_limit\n",
            "Saving model checkpoint to /path/to/output/dir/for/training/arguments/checkpoint-700\n",
            "Configuration saved in /path/to/output/dir/for/training/arguments/checkpoint-700/config.json\n",
            "Model weights saved in /path/to/output/dir/for/training/arguments/checkpoint-700/pytorch_model.bin\n",
            "Deleting older checkpoint [/path/to/output/dir/for/training/arguments/checkpoint-500] due to args.save_total_limit\n",
            "Saving model checkpoint to /path/to/output/dir/for/training/arguments/checkpoint-800\n",
            "Configuration saved in /path/to/output/dir/for/training/arguments/checkpoint-800/config.json\n",
            "Model weights saved in /path/to/output/dir/for/training/arguments/checkpoint-800/pytorch_model.bin\n",
            "Deleting older checkpoint [/path/to/output/dir/for/training/arguments/checkpoint-600] due to args.save_total_limit\n",
            "Saving model checkpoint to /path/to/output/dir/for/training/arguments/checkpoint-900\n",
            "Configuration saved in /path/to/output/dir/for/training/arguments/checkpoint-900/config.json\n",
            "Model weights saved in /path/to/output/dir/for/training/arguments/checkpoint-900/pytorch_model.bin\n",
            "Deleting older checkpoint [/path/to/output/dir/for/training/arguments/checkpoint-700] due to args.save_total_limit\n",
            "Saving model checkpoint to /path/to/output/dir/for/training/arguments/checkpoint-1000\n",
            "Configuration saved in /path/to/output/dir/for/training/arguments/checkpoint-1000/config.json\n",
            "Model weights saved in /path/to/output/dir/for/training/arguments/checkpoint-1000/pytorch_model.bin\n",
            "Deleting older checkpoint [/path/to/output/dir/for/training/arguments/checkpoint-800] due to args.save_total_limit\n",
            "Saving model checkpoint to /path/to/output/dir/for/training/arguments/checkpoint-1100\n",
            "Configuration saved in /path/to/output/dir/for/training/arguments/checkpoint-1100/config.json\n",
            "Model weights saved in /path/to/output/dir/for/training/arguments/checkpoint-1100/pytorch_model.bin\n",
            "Deleting older checkpoint [/path/to/output/dir/for/training/arguments/checkpoint-900] due to args.save_total_limit\n",
            "Saving model checkpoint to /path/to/output/dir/for/training/arguments/checkpoint-1200\n",
            "Configuration saved in /path/to/output/dir/for/training/arguments/checkpoint-1200/config.json\n",
            "Model weights saved in /path/to/output/dir/for/training/arguments/checkpoint-1200/pytorch_model.bin\n",
            "Deleting older checkpoint [/path/to/output/dir/for/training/arguments/checkpoint-1000] due to args.save_total_limit\n",
            "Saving model checkpoint to /path/to/output/dir/for/training/arguments/checkpoint-1300\n",
            "Configuration saved in /path/to/output/dir/for/training/arguments/checkpoint-1300/config.json\n",
            "Model weights saved in /path/to/output/dir/for/training/arguments/checkpoint-1300/pytorch_model.bin\n",
            "Deleting older checkpoint [/path/to/output/dir/for/training/arguments/checkpoint-1100] due to args.save_total_limit\n",
            "Saving model checkpoint to /path/to/output/dir/for/training/arguments/checkpoint-1400\n",
            "Configuration saved in /path/to/output/dir/for/training/arguments/checkpoint-1400/config.json\n",
            "Model weights saved in /path/to/output/dir/for/training/arguments/checkpoint-1400/pytorch_model.bin\n",
            "Deleting older checkpoint [/path/to/output/dir/for/training/arguments/checkpoint-1200] due to args.save_total_limit\n",
            "Saving model checkpoint to /path/to/output/dir/for/training/arguments/checkpoint-1500\n",
            "Configuration saved in /path/to/output/dir/for/training/arguments/checkpoint-1500/config.json\n",
            "Model weights saved in /path/to/output/dir/for/training/arguments/checkpoint-1500/pytorch_model.bin\n",
            "Deleting older checkpoint [/path/to/output/dir/for/training/arguments/checkpoint-1300] due to args.save_total_limit\n",
            "Saving model checkpoint to /path/to/output/dir/for/training/arguments/checkpoint-1600\n",
            "Configuration saved in /path/to/output/dir/for/training/arguments/checkpoint-1600/config.json\n",
            "Model weights saved in /path/to/output/dir/for/training/arguments/checkpoint-1600/pytorch_model.bin\n",
            "Deleting older checkpoint [/path/to/output/dir/for/training/arguments/checkpoint-1400] due to args.save_total_limit\n",
            "Saving model checkpoint to /path/to/output/dir/for/training/arguments/checkpoint-1700\n",
            "Configuration saved in /path/to/output/dir/for/training/arguments/checkpoint-1700/config.json\n",
            "Model weights saved in /path/to/output/dir/for/training/arguments/checkpoint-1700/pytorch_model.bin\n",
            "Deleting older checkpoint [/path/to/output/dir/for/training/arguments/checkpoint-1500] due to args.save_total_limit\n",
            "Saving model checkpoint to /path/to/output/dir/for/training/arguments/checkpoint-1800\n",
            "Configuration saved in /path/to/output/dir/for/training/arguments/checkpoint-1800/config.json\n",
            "Model weights saved in /path/to/output/dir/for/training/arguments/checkpoint-1800/pytorch_model.bin\n",
            "Deleting older checkpoint [/path/to/output/dir/for/training/arguments/checkpoint-1600] due to args.save_total_limit\n",
            "Saving model checkpoint to /path/to/output/dir/for/training/arguments/checkpoint-1900\n",
            "Configuration saved in /path/to/output/dir/for/training/arguments/checkpoint-1900/config.json\n",
            "Model weights saved in /path/to/output/dir/for/training/arguments/checkpoint-1900/pytorch_model.bin\n",
            "Deleting older checkpoint [/path/to/output/dir/for/training/arguments/checkpoint-1700] due to args.save_total_limit\n",
            "Saving model checkpoint to /path/to/output/dir/for/training/arguments/checkpoint-2000\n",
            "Configuration saved in /path/to/output/dir/for/training/arguments/checkpoint-2000/config.json\n",
            "Model weights saved in /path/to/output/dir/for/training/arguments/checkpoint-2000/pytorch_model.bin\n",
            "Deleting older checkpoint [/path/to/output/dir/for/training/arguments/checkpoint-1800] due to args.save_total_limit\n",
            "Saving model checkpoint to /path/to/output/dir/for/training/arguments/checkpoint-2100\n",
            "Configuration saved in /path/to/output/dir/for/training/arguments/checkpoint-2100/config.json\n",
            "Model weights saved in /path/to/output/dir/for/training/arguments/checkpoint-2100/pytorch_model.bin\n",
            "Deleting older checkpoint [/path/to/output/dir/for/training/arguments/checkpoint-1900] due to args.save_total_limit\n",
            "Saving model checkpoint to /path/to/output/dir/for/training/arguments/checkpoint-2200\n",
            "Configuration saved in /path/to/output/dir/for/training/arguments/checkpoint-2200/config.json\n",
            "Model weights saved in /path/to/output/dir/for/training/arguments/checkpoint-2200/pytorch_model.bin\n",
            "Deleting older checkpoint [/path/to/output/dir/for/training/arguments/checkpoint-2000] due to args.save_total_limit\n",
            "Saving model checkpoint to /path/to/output/dir/for/training/arguments/checkpoint-2300\n",
            "Configuration saved in /path/to/output/dir/for/training/arguments/checkpoint-2300/config.json\n",
            "Model weights saved in /path/to/output/dir/for/training/arguments/checkpoint-2300/pytorch_model.bin\n",
            "Deleting older checkpoint [/path/to/output/dir/for/training/arguments/checkpoint-2100] due to args.save_total_limit\n",
            "Saving model checkpoint to /path/to/output/dir/for/training/arguments/checkpoint-2400\n",
            "Configuration saved in /path/to/output/dir/for/training/arguments/checkpoint-2400/config.json\n",
            "Model weights saved in /path/to/output/dir/for/training/arguments/checkpoint-2400/pytorch_model.bin\n",
            "Deleting older checkpoint [/path/to/output/dir/for/training/arguments/checkpoint-2200] due to args.save_total_limit\n",
            "Saving model checkpoint to /path/to/output/dir/for/training/arguments/checkpoint-2500\n",
            "Configuration saved in /path/to/output/dir/for/training/arguments/checkpoint-2500/config.json\n",
            "Model weights saved in /path/to/output/dir/for/training/arguments/checkpoint-2500/pytorch_model.bin\n",
            "Deleting older checkpoint [/path/to/output/dir/for/training/arguments/checkpoint-2300] due to args.save_total_limit\n",
            "Saving model checkpoint to /path/to/output/dir/for/training/arguments/checkpoint-2600\n",
            "Configuration saved in /path/to/output/dir/for/training/arguments/checkpoint-2600/config.json\n",
            "Model weights saved in /path/to/output/dir/for/training/arguments/checkpoint-2600/pytorch_model.bin\n",
            "Deleting older checkpoint [/path/to/output/dir/for/training/arguments/checkpoint-2400] due to args.save_total_limit\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-6b841d6f9521>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m )\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"path/to/your/model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1523\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1524\u001b[0m             \u001b[0mtrial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1525\u001b[0;31m             \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1526\u001b[0m         )\n\u001b[1;32m   1527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1766\u001b[0m                     \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogging_nan_inf_filter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1767\u001b[0m                     \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_torch_tpu_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1768\u001b[0;31m                     \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss_step\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misinf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtr_loss_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1769\u001b[0m                 ):\n\u001b[1;32m   1770\u001b[0m                     \u001b[0;31m# if loss is nan or inf simply add the average of previous logged losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "x = torch.randn( 8, 70)\n",
        "\n",
        "x.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4TAUpLl5z5Rw",
        "outputId": "709afb8f-8526-428b-f06a-ce19607abb90"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 70])"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fVxQyNN7E7CC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = torch.ones([8])\n",
        "\n",
        "y[2]=0\n",
        "y[3]=0\n",
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPi65-SH102G",
        "outputId": "8f49526b-5291-4097-e920-1bba54b131a7"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8])"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mw4tqLA888bn",
        "outputId": "0f540035-dec8-4b02-db22-b481bb8b6f6a"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.],\n",
              "        [1.],\n",
              "        [0.],\n",
              "        [0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "z= torch.mul(x, y)"
      ],
      "metadata": {
        "id": "KcCxsG0b7lrF"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "z.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4lj_Y6Ju6bzT",
        "outputId": "b0466d32-5ca6-4e46-8408-7bf53602cfa7"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(y)):\n",
        "  if y[i] == 0:\n",
        "    x[i]= x[i]*0 - 100\n",
        "    \n",
        "    "
      ],
      "metadata": {
        "id": "Rqj9H-AK61GR"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x[3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UznGWLM17dn1",
        "outputId": "36c646da-aa3c-46df-9891-827e31981865"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-100., -100., -100., -100., -100., -100., -100., -100., -100., -100.,\n",
              "        -100., -100., -100., -100., -100., -100., -100., -100., -100., -100.,\n",
              "        -100., -100., -100., -100., -100., -100., -100., -100., -100., -100.,\n",
              "        -100., -100., -100., -100., -100., -100., -100., -100., -100., -100.,\n",
              "        -100., -100., -100., -100., -100., -100., -100., -100., -100., -100.,\n",
              "        -100., -100., -100., -100., -100., -100., -100., -100., -100., -100.,\n",
              "        -100., -100., -100., -100., -100., -100., -100., -100., -100., -100.])"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x.view(-1, 2).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZJ6XTbo15lc",
        "outputId": "0f3bfeaa-0aa4-42b0-b463-955b31121bfe"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "XZWkYEvS170l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}